# ä¼˜åŒ–äº†UIè®¾ç½®ï¼Œä½†æ˜¯æ²¡æœ‰å®ç°ï¼š1.ä¿å­˜å¯†ç  2.ä¿®å¤æ–‡ä»¶å¤„ç†bug
import streamlit as st
import os
import requests
import easyocr
from PIL import Image
import io
import fitz  # PyMuPDF
import json
from groq import Groq
import openai
from openai import OpenAI
import time
import streamlit.components.v1 as components
import numpy as np

deepseek_url = "https://api.deepseek.com"
silicon_url = "https://api.siliconflow.cn/v1/"
groq_api_key = ""
# åˆå§‹åŒ–EasyOCRé˜…è¯»å™¨
reader = easyocr.Reader(['ch_sim', 'en'])

# é¡µé¢é…ç½®
st.set_page_config(page_title="ASTERISKY AI", layout="wide")

# åˆå§‹åŒ–sessionçŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "ocr_text" not in st.session_state:
    st.session_state.ocr_text = ""
if "context" not in st.session_state:
    st.session_state.context = None

# ä¿å­˜


# æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
uploaded_file = None

# åˆ›å»ºä¸€ä¸ªå®¹å™¨ï¼Œç”¨äºæ”¾ç½®æ ‡é¢˜å’Œæ–‡ä»¶ä¸Šä¼ æŒ‰é’®
with st.container():
        st.markdown(
            """
            <div style="position: sticky; top: 0; background-color: white; z-index: 999;">
                <h1 style="margin: 0;">âœ³ï¸ ASTERISKY STUDIO</h1>
            </div>
            """,
            unsafe_allow_html=True,
        )

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("APIè®¾ç½®")
    selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", ["Groq", "Ollama", "DeepSeek", "SiliconCloud"])
    # æ·»åŠ Ollamaæ¨¡å‹åç§°è¾“å…¥
    
    if selected_model == "Ollama":
        ollama_api_url = st.text_input("Ollama APIåœ°å€", "http://localhost:11434/api/generate")
        ollama_model_name = st.text_input("Ollamaæ¨¡å‹åç§°", "deepseek-r1:8b")
        ollama_model_name = "deepseek-r1:8b"  # é»˜è®¤å€¼
    
    if selected_model == "SiliconCloud":
        silicon_model_name = st.text_input("SiliconCloudæ¨¡å‹åç§°", "deepseek-ai/DeepSeek-R1")
        silicon_api_key = st.text_input("SiliconCloud APIå¯†é’¥", type="password")
        silicon_model_name = "deepseek-ai/DeepSeek-R1"  # é»˜è®¤å€¼

    if selected_model == "DeepSeek":
        deepseek_api_key = st.text_input("DeepSeek APIå¯†é’¥", type="password")
    if selected_model == "Groq":
        groq_api_key = st.text_input("Groq APIå¯†é’¥", type="password")
    
    # æ¸…é™¤ä¸Šä¸‹æ–‡æŒ‰é’®
    st.header("åŠŸèƒ½")
    if st.button("ğŸ§¹æ¸…é™¤ä¸Šä¸‹æ–‡"):
        st.session_state.context = None
        st.session_state.messages = []
        st.session_state.ocr_text = ""
        st.success("ä¸Šä¸‹æ–‡å·²æ¸…é™¤")
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡æˆ–PDF", type=["png", "jpg", "jpeg", "pdf"])
    st.header("å‚æ•°è®¾ç½®")
    temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.7)
    system = st.text_area("ç³»ç»Ÿæç¤º", "")

# æŠŠAPIå¯†é’¥è®¾ç½®åˆ°ç¯å¢ƒå˜é‡ä¸­
os.environ["GROQ_API_KEY"] = groq_api_key

# æ–‡ä»¶å¤„ç†å‡½æ•°
def process_file(uploaded_file):
    if uploaded_file is None:
        return ""
    
    # å¤„ç†PDF
    if uploaded_file.type == "application/pdf":
        doc = fitz.open(stream=uploaded_file.read())
        text = ""
        for page in doc:
            pix = page.get_pixmap()
            img = Image.open(io.BytesIO(pix.tobytes()))
            result = reader.readtext(img)
            text += "\n".join([res[1] for res in result]) + "\n"
        return text

    # å¤„ç†å›¾ç‰‡
    else:
        img = Image.open(uploaded_file)
        result = reader.readtext(img)
        return "\n".join([res[1] for res in result])

# æµå¼APIè°ƒç”¨å‡½æ•°
def stream_ollama_response(prompt, model_name):
    data = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "stream": True,
    }
    if st.session_state.context is not None:
        data["context"] = st.session_state.context

    try:
        with requests.post(
            ollama_api_url,
            json=data,
            stream=True,
            timeout=60
        ) as response:
            response.raise_for_status()
            
            full_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    if 'response' in chunk:
                        yield chunk['response']
                    if chunk.get('done'):
                        st.session_state.context = chunk.get('context')

    except requests.exceptions.RequestException as e:
        yield f"è¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    except json.JSONDecodeError as e:
        yield f"è§£æå“åº”æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    except Exception as e:
        yield f"å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}"

# Groq APIè°ƒç”¨å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
def call_groq_api(prompt, client):
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = prompt

        # è°ƒç”¨ API è·å–å›å¤
        chat_completion = client.chat.completions.create(
            messages=st.session_state.messages,
            model="deepseek-r1-distill-llama-70b"
        )

        # æå–åŠ©æ‰‹å›å¤å†…å®¹
        assistant_reply = chat_completion.choices[0].message.content

        # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ï¼Œä»¥æ”¯æŒå¤šè½®å¯¹è¯
        assistant_message = {
            "role": "assistant",
            "content": assistant_reply
        }
        st.session_state.messages.append(assistant_message)
        return assistant_reply

    except Exception as e:
        print(f"An error occurred: {e}")

# ä¿®æ­£åçš„DeepSeek APIè°ƒç”¨å‡½æ•°
def call_deepseek_api(prompt, client):
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = prompt
        # è°ƒç”¨ API è¿›è¡Œæµå¼å¯¹è¯
        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=st.session_state.messages,
            stream=True
        )
        return response
    except Exception as e:
        return f"DeepSeek APIè¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# SiliconCloud APIè°ƒç”¨å‡½æ•°
def call_silicon_api(prompt, client):
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = prompt
        # è°ƒç”¨ API è¿›è¡Œæµå¼å¯¹è¯
        response = client.chat.completions.create(
            model=silicon_model_name,
            messages=st.session_state.messages,
            stream=True
        )
        return response
    except Exception as e:
        return f"SiliconCloud APIè¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
# ä¸»å‡½æ•°
def main():
    # ä¸»ç•Œé¢
    st.title(f"âœ¨ AI Chat ğŸ¤— {selected_model}")


    # OCRå¤„ç†
    if uploaded_file:
        with st.spinner("æ­£åœ¨è§£ææ–‡ä»¶..."):
            st.session_state.ocr_text = process_file(uploaded_file)
        st.text_area("è§£æç»“æœ", st.session_state.ocr_text, height=150)

    # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œç‚¹å‡»åè§¦å‘æ–‡ä»¶ä¸Šä¼ 

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ç”¨æˆ·è¾“å…¥å¤„ç†
    if prompt := st.chat_input("è¾“å…¥æ¶ˆæ¯..."):
        final_prompt = st.session_state.ocr_text if st.session_state.ocr_text else prompt
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": final_prompt})
        with st.chat_message("user"):
            st.markdown(final_prompt)
        
        # å‡†å¤‡ç”Ÿæˆå›å¤
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è°ƒç”¨ä¸åŒçš„API
            if selected_model == "Ollama":
                for chunk in stream_ollama_response(final_prompt, ollama_model_name):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")

            elif selected_model == "Groq":
                client = Groq(api_key=os.environ.get("GROQ_API_KEY"))
                for chunk in call_groq_api(final_prompt, client):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")

            elif selected_model == "DeepSeek":
                client = OpenAI(api_key=silicon_api_key, base_url=silicon_url)
                try:
                    # æ¨ç†
                    response = call_deepseek_api(final_prompt, client)
                    full_response += "<think>"
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta,'reasoning_content') and delta.reasoning_content is not None:
                            reasoning_chunk = delta.reasoning_content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += reasoning_chunk
                        else:
                            break
                    full_response += r"<\think>"
                    if hasattr(delta, 'content') and delta.content is not None:
                        content_chunk = delta.content
                        full_response += content_chunk
                        response_placeholder.markdown(full_response + "â–Œ")
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content is not None:
                            content_chunk = delta.content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += content_chunk
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"å‘ç”Ÿé”™è¯¯: {e}, å¯èƒ½æ˜¯ DeepSeek API ç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            
            elif selected_model == "SiliconCloud":
                client = OpenAI(api_key=silicon_api_key, base_url=silicon_url)
                try:
                    # æ¨ç†
                    response = call_silicon_api(final_prompt, client)
                    full_response += "<think>"
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta,'reasoning_content') and delta.reasoning_content is not None:
                            reasoning_chunk = delta.reasoning_content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += reasoning_chunk
                        else:
                            break
                    full_response += r"<\think>"
                    if hasattr(delta, 'content') and delta.content is not None:
                        content_chunk = delta.content
                        full_response += content_chunk
                        response_placeholder.markdown(full_response + "â–Œ")
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content is not None:
                            content_chunk = delta.content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += content_chunk
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"å‘ç”Ÿé”™è¯¯: {e}, å¯èƒ½æ˜¯ Silicon API ç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            
            # æœ€ç»ˆæ›´æ–°å»æ‰å…‰æ ‡
            response_placeholder.markdown(full_response)
        
        # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
        if selected_model == "Ollama":
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
