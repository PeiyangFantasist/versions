import streamlit as st
import os
import requests
import easyocr
from PIL import Image
import io
import fitz  # PyMuPDF
import json
from groq import Groq
import openai
from openai import OpenAI
import time
import streamlit.components.v1 as components
import clipboard
import numpy as np
import modalhtml as mh
import to_text as tx
import screenshot as sc
import threading

deepseek_url = "https://api.deepseek.com"
silicon_url = "https://api.siliconflow.cn/v1/"
groq_api_key = ""


# é¡µé¢é…ç½®
st.set_page_config(page_title="ASTERISKY AI", layout="wide")

# åˆå§‹åŒ–sessionçŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "ocr_text" not in st.session_state:
    st.session_state.ocr_text = ""
if "context" not in st.session_state:
    st.session_state.context = None

# æ–°å¢ï¼šåˆå§‹åŒ–ä¿å­˜ API å¯†é’¥çš„ session çŠ¶æ€
if "groq_api_key" not in st.session_state:
    st.session_state.groq_api_key = ""
if "ollama_api_url" not in st.session_state:
    st.session_state.ollama_api_url = "http://localhost:11434/api/generate"
if "ollama_model_name" not in st.session_state:
    st.session_state.ollama_model_name = "deepseek-r1:8b"
if "silicon_model_name" not in st.session_state:
    st.session_state.silicon_model_name = "deepseek-ai/DeepSeek-R1"
if "silicon_api_key" not in st.session_state:
    st.session_state.silicon_api_key = ""
if "deepseek_api_key" not in st.session_state:
    st.session_state.deepseek_api_key = ""

# ä¿å­˜


# æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
uploaded_file = None
# åˆ›å»ºä¸€ä¸ªå®¹å™¨ï¼Œç”¨äºæ”¾ç½®æ ‡é¢˜å’Œæ–‡ä»¶ä¸Šä¼ æŒ‰é’®
with st.container():
        st.markdown(
            """
            <div style="position: sticky; top: 0; background-color: white; z-index: 999;">
                <h1 style="margin: 0;">âœ³ï¸ ASTERISKY STUDIO</h1>
            </div>
            """,
            unsafe_allow_html=True,
        )

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("ğŸ”‘ APIè®¾ç½®")
    selected_model = st.selectbox("é€‰æ‹©æ¥å£", ["Groq", "Ollama", "DeepSeek", "SiliconCloud"])
    # æ·»åŠ Ollamaæ¨¡å‹åç§°è¾“å…¥
    
    if selected_model == "Ollama":
        # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¿å­˜ç”¨æˆ·è¾“å…¥
        st.session_state.ollama_api_url = st.text_input("Ollama APIåœ°å€", st.session_state.ollama_api_url)
        st.session_state.ollama_model_name = st.text_input("Ollamaæ¨¡å‹åç§°", st.session_state.ollama_model_name)
    
    if selected_model == "SiliconCloud":
        # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¿å­˜ç”¨æˆ·è¾“å…¥
        st.session_state.silicon_model_name = st.text_input("SiliconCloudæ¨¡å‹åç§°", st.session_state.silicon_model_name)
        st.session_state.silicon_api_key = st.text_input("SiliconCloud APIå¯†é’¥", type="password", value=st.session_state.silicon_api_key)
    
    if selected_model == "DeepSeek":
        # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¿å­˜ç”¨æˆ·è¾“å…¥
        st.session_state.deepseek_api_key = st.text_input("DeepSeek APIå¯†é’¥", type="password", value=st.session_state.deepseek_api_key)
    if selected_model == "Groq":
        # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¿å­˜ç”¨æˆ·è¾“å…¥
        st.session_state.groq_api_key = st.text_input("Groq APIå¯†é’¥", type="password", value=st.session_state.groq_api_key)
    
    # æ¸…é™¤ä¸Šä¸‹æ–‡æŒ‰é’®
    st.header("âš™ï¸ å¸¸ç”¨åŠŸèƒ½")
    with st.expander("å¸®åŠ©", expanded=False):
        st.write("ç”¨æˆ·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶ï¼Œå¿…é¡»éµå®ˆä¸­åäººæ°‘å…±å’Œå›½ç›¸å…³æ³•å¾‹æ³•è§„ï¼Œä»¥ä¸‹æ˜¯å¸®åŠ©æ–‡æ¡£")
        st.markdown("""
        # å–œæŠ¥
        ## æˆ‘è¿˜æ²¡å†™
        """)
    if st.button("ğŸ§¹ æ¸…é™¤ä¸Šä¸‹æ–‡"):
        st.session_state.context = None
        st.session_state.messages = []
        st.session_state.ocr_text = ""
        st.success("ä¸Šä¸‹æ–‡å·²æ¸…é™¤ãƒ¾(â‰§â–½â‰¦*)o")
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.header("ğŸ”  å¿«æ·è½¬æ–‡å­—") 
    language = st.selectbox("é€‰æ‹©æ–‡å­—è¯†åˆ«ç±»å‹", ['ä¸­æ–‡', 'è‹±æ–‡', 'ä¸­è‹±æ··åˆ', 'æ•°å­¦å…¬å¼'])
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡è½¬æ–‡å­—", type=["png", "jpg", "jpeg", "pdf"])
    if st.button('âœ‚ï¸ æˆªå›¾è½¬æ–‡å­—'):
        def handle_screenshot():
            global uploaded_file
            # è°ƒç”¨æˆªå›¾å‡½æ•°
            screenshot_file = sc.open_screenshot_window()
            if screenshot_file:
                # å°†æˆªå›¾æ•°æ®å°è£…æˆç±»ä¼¼ st.file_uploader è¿”å›çš„æ–‡ä»¶å¯¹è±¡
                buffer = io.BytesIO(screenshot_file._bytes_data)
                buffer.name = "screenshot.png"  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹æ–‡ä»¶å
                buffer.type = "image/png"  # å‡è®¾æˆªå›¾æ˜¯ PNG æ ¼å¼ï¼Œå¯æŒ‰éœ€ä¿®æ”¹
                uploaded_file = buffer
                # è¿›è¡Œæ–‡ä»¶å¤„ç†ï¼Œä¾‹å¦‚è°ƒç”¨ OCR å‡½æ•°è¿›è¡Œæ–‡å­—è¯†åˆ«
                if uploaded_file:
                    if uploaded_file.type == "application/pdf":
                        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                        text = ""
                        for page_num in range(len(pdf_document)):
                            page = pdf_document.load_page(page_num)
                            text += page.get_text()
                    else:
                        image = Image.open(uploaded_file)
                        reader = easyocr.Reader(['ch_sim', 'en'])
                        text = reader.readtext(np.array(image), detail=0)
                    st.session_state.ocr_text = text
                    st.write("è¯†åˆ«ç»“æœï¼š", text)

        threading.Thread(target=handle_screenshot).start()
    # ä½¿ç”¨çº¿ç¨‹æ¥é¿å…é˜»å¡ Streamlit åº”ç”¨

    st.header("å‚æ•°è®¾ç½®")
    temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.7)
    system = st.text_area("ç³»ç»Ÿæç¤º", "")

# æŠŠAPIå¯†é’¥è®¾ç½®åˆ°ç¯å¢ƒå˜é‡ä¸­
# ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¸­çš„å€¼
os.environ["GROQ_API_KEY"] = st.session_state.groq_api_key

# æ–‡ä»¶å¤„ç†å‡½æ•°


# æµå¼APIè°ƒç”¨å‡½æ•°
def stream_ollama_response(prompt, model_name):
    data = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "stream": True,
    }
    if st.session_state.context is not None:
        data["context"] = st.session_state.context

    try:
        with requests.post(
            st.session_state.ollama_api_url,  # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¸­çš„å€¼
            json=data,
            stream=True,
            timeout=60
        ) as response:
            response.raise_for_status()
            
            full_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    if 'response' in chunk:
                        yield chunk['response']
                    if chunk.get('done'):
                        st.session_state.context = chunk.get('context')

    except requests.exceptions.RequestException as e:
        yield f"è¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    except json.JSONDecodeError as e:
        yield f"è§£æå“åº”æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    except Exception as e:
        yield f"å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}"

# Groq APIè°ƒç”¨å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
def call_groq_api(prompt, client):
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = prompt

        # è°ƒç”¨ API è·å–å›å¤
        chat_completion = client.chat.completions.create(
            messages=st.session_state.messages,
            model="deepseek-r1-distill-llama-70b"
        )

        # æå–åŠ©æ‰‹å›å¤å†…å®¹
        assistant_reply = chat_completion.choices[0].message.content

        # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ï¼Œä»¥æ”¯æŒå¤šè½®å¯¹è¯
        assistant_message = {
            "role": "assistant",
            "content": assistant_reply
        }
        st.session_state.messages.append(assistant_message)
        return assistant_reply

    except Exception as e:
        print(f"An error occurred: {e}")

# ä¿®æ­£åçš„DeepSeek APIè°ƒç”¨å‡½æ•°
def call_deepseek_api(prompt, client):
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = prompt
        # è°ƒç”¨ API è¿›è¡Œæµå¼å¯¹è¯
        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=st.session_state.messages,
            stream=True
        )
        return response
    except Exception as e:
        return f"DeepSeek APIè¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# SiliconCloud APIè°ƒç”¨å‡½æ•°
def call_silicon_api(prompt, client):
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = prompt
        # è°ƒç”¨ API è¿›è¡Œæµå¼å¯¹è¯
        response = client.chat.completions.create(
            model=st.session_state.silicon_model_name,  # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¸­çš„å€¼
            messages=st.session_state.messages,
            stream=True
        )
        return response
    except Exception as e:
        return f"SiliconCloud APIè¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# ä¸»å‡½æ•°
def main():
    # ä¸»ç•Œé¢
    st.title(f"âœ¨ AI Chat ğŸ¤— {selected_model}")


    # OCRå¤„ç†
    if uploaded_file:
        with st.spinner("æ­£åœ¨è§£ææ–‡ä»¶..."):
            st.session_state.ocr_text = tx.process_file(uploaded_file, language)
        st.text_area("è§£æç»“æœ(ç‚¹å‡»å·¦ä¸‹è§’å…³é—­æ–‡ä»¶å³å¯å…³é—­è§£æ)", st.session_state.ocr_text, height=150)
            
        if st.button("ğŸ“‹ å¤åˆ¶è§£æç»“æœ"):
            clipboard.copy(st.session_state.ocr_text)
            st.success("è§£æç»“æœå·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼o((>Ï‰< ))o")
    # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œç‚¹å‡»åè§¦å‘æ–‡ä»¶ä¸Šä¼ 

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ç”¨æˆ·è¾“å…¥å¤„ç†
    if prompt := st.chat_input("è¾“å…¥æ¶ˆæ¯..."):
        final_prompt = st.session_state.ocr_text if st.session_state.ocr_text else prompt
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": final_prompt})
        with st.chat_message("user"):
            st.markdown(final_prompt)
        
        # å‡†å¤‡ç”Ÿæˆå›å¤
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è°ƒç”¨ä¸åŒçš„API
            if selected_model == "Ollama":
                for chunk in stream_ollama_response(final_prompt, st.session_state.ollama_model_name):  # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¸­çš„å€¼
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")

            elif selected_model == "Groq":
                client = Groq(api_key=os.environ.get("GROQ_API_KEY"))
                for chunk in call_groq_api(final_prompt, client):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")

            elif selected_model == "DeepSeek":
                client = OpenAI(api_key=st.session_state.deepseek_api_key, base_url=deepseek_url)  # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¸­çš„å€¼
                try:
                    # æ¨ç†
                    response = call_deepseek_api(final_prompt, client)
                    full_response += "<think>"
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta,'reasoning_content') and delta.reasoning_content is not None:
                            reasoning_chunk = delta.reasoning_content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += reasoning_chunk
                        else:
                            break
                    full_response += r"<\think>"
                    if hasattr(delta, 'content') and delta.content is not None:
                        content_chunk = delta.content
                        full_response += content_chunk
                        response_placeholder.markdown(full_response + "â–Œ")
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content is not None:
                            content_chunk = delta.content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += content_chunk
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"å‘ç”Ÿé”™è¯¯: {e}, å¯èƒ½æ˜¯ DeepSeek API ç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            
            elif selected_model == "SiliconCloud":
                client = OpenAI(api_key=st.session_state.silicon_api_key, base_url=silicon_url)  # ä¿®æ”¹ï¼šä½¿ç”¨ session_state ä¸­çš„å€¼
                try:
                    # æ¨ç†
                    response = call_silicon_api(final_prompt, client)
                    full_response += "<think>"
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta,'reasoning_content') and delta.reasoning_content is not None:
                            reasoning_chunk = delta.reasoning_content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += reasoning_chunk
                        else:
                            break
                    full_response += r"<\think>"
                    if hasattr(delta, 'content') and delta.content is not None:
                        content_chunk = delta.content
                        full_response += content_chunk
                        response_placeholder.markdown(full_response + "â–Œ")
                    for chunk in response:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content is not None:
                            content_chunk = delta.content
                            response_placeholder.markdown(full_response + "â–Œ")
                            full_response += content_chunk
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"å‘ç”Ÿé”™è¯¯: {e}, å¯èƒ½æ˜¯ Silicon API ç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            
            # æœ€ç»ˆæ›´æ–°å»æ‰å…‰æ ‡
            response_placeholder.markdown(full_response)
        
        # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
        if selected_model == "Ollama":
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
